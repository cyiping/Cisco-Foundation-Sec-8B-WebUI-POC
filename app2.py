# ==========================================
# æ­¥é©Ÿ 1: å®‰è£ç’°å¢ƒ (ä¿®æ­£ç‰ˆ)
# ==========================================
import subprocess
import sys

def install_env():
    print("æ­£åœ¨æº–å‚™ç’°å¢ƒï¼Œè«‹ç¨å€™...")
    # é—œéµä¿®æ­£ï¼šç§»é™¤ 'torch'ï¼Œä½¿ç”¨ Kaggle å…§å»ºç‰ˆæœ¬é¿å…ç‰ˆæœ¬è¡çª
    packages = ["transformers", "accelerate", "bitsandbytes", "gradio"]
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q"] + packages)
    print("ç’°å¢ƒå®‰è£å®Œæˆï¼")

install_env()

# ==========================================
# æ­¥é©Ÿ 2: è¼‰å…¥æ¨¡çµ„
# ==========================================
import os
import time
import csv
import torch
import gradio as gr
from datetime import datetime
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    pipeline, 
    BitsAndBytesConfig
)

# ==========================================
# æ­¥é©Ÿ 3: æ¨¡å‹åˆå§‹åŒ– (é‡å° T4 å„ªåŒ–)
# ==========================================
MODEL_ID = "fdtn-ai/Foundation-Sec-8B"
print(f"æ­£åœ¨è¼‰å…¥æ¨¡å‹: {MODEL_ID} ...")

# è¨­å®š 4-bit é‡åŒ– (é—œéµï¼šè®“ 16GB VRAM çš„ T4 è·‘å¾—å‹•ä¸”ä¸çˆ†é¡¯å­˜)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

try:
    # 1. è¼‰å…¥ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    # ä¿®æ­£ Llama æ¶æ§‹å¸¸è¦‹çš„ padding å•é¡Œ
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # 2. è¼‰å…¥æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        low_cpu_mem_usage=True
    )

    # 3. å»ºç«‹ Pipeline
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        return_full_text=False # åªå›å‚³ç”Ÿæˆçš„å…§å®¹
    )
    print("æ¨¡å‹è¼‰å…¥æˆåŠŸï¼")

except Exception as e:
    print(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
    # è‹¥æ˜¯ç§æœ‰æ¨¡å‹éœ€è¦ loginï¼Œè«‹å–æ¶ˆè¨»è§£ä¸‹è¡Œ
    # from huggingface_hub import login; login()
    raise e

WORKING_DIR = "/kaggle/working"
LOG_FILE = os.path.join(WORKING_DIR, "security_log.csv")

# ==========================================
# æ­¥é©Ÿ 4: å®šç¾©ç”Ÿæˆé‚è¼¯ (å·²åŠ å…¥ Penalty)
# ==========================================
def generate_security_analysis(prompt, max_tokens, temperature, top_p):
    start_time = time.time()
    try:
        # 4.1 åŠ å…¥ System Prompt å¼·åŒ–è§’è‰²è¨­å®š
        system_prompt = "You are a senior cyber security Engineer. Analyze the vulnerability concisely."
        full_prompt = f"{system_prompt}\n\nTask: {prompt}\n\nAnswer:"

        # 4.2 åŸ·è¡Œç”Ÿæˆ (åŠ å…¥ repetition_penalty)
        output = pipe(
            full_prompt, 
            max_new_tokens=max_tokens, 
            do_sample=True, 
            temperature=temperature,
            top_p=top_p,
            pad_token_id=tokenizer.eos_token_id,
            # ---------------------------------------------------
            # â˜… é—œéµä¿®æ”¹ï¼šåŠ å…¥é‡è¤‡æ‡²ç½°ï¼Œæ•¸å€¼å»ºè­° 1.1 ~ 1.2
            # ---------------------------------------------------
            repetition_penalty=1.15 
        )
        
        response = output[0]['generated_text'].strip()
        
        # 4.3 è¨˜éŒ„ Log
        duration = time.time() - start_time
        file_exists = os.path.isfile(LOG_FILE)
        with open(LOG_FILE, mode='a', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            if not file_exists:
                writer.writerow(["Timestamp", "Prompt", "Response", "Duration"])
            writer.writerow([datetime.now(), prompt, response, f"{duration:.2f}"])
            
        return response
    except Exception as e:
        return f"ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

# ==========================================
# æ­¥é©Ÿ 5: å»ºç«‹ä»‹é¢
# ==========================================

custom_css = """
.eta-stats {
    font-size: 30px !important;      
    line-height: 2.5 !important;     
    font-weight: bold !important;
    background: #FFF9C4 !important;  
    border: 4px solid #FBC02D !important;
    border-radius: 15px !important;
    padding: 10px 30px !important;
}
.generating {
    transform: scale(2.2) !important; 
    margin-right: 25px !important;
}
.title_center { text-align: center; }
.hint-text { font-size: 0.85em; color: #555; margin-bottom: 2px; }
"""

with gr.Blocks(title="Cisco Security Model POC", theme=gr.themes.Default(), css=custom_css) as demo:
    gr.Markdown("# ğŸ›¡ï¸ Cisco Security Model POC (Anti-Loop Fix)", elem_classes="title_center")
    
    with gr.Row():
        # å·¦å´ï¼šè¼¸å…¥èˆ‡åƒæ•¸è¨­å®š
        with gr.Column(scale=1):
            input_box = gr.Textbox(
                label="Security Prompt (æå•)", 
                placeholder="ä¾‹å¦‚ï¼šanalysis CVE-2022-21540...",
                lines=8
            )
            
            with gr.Accordion("âš™ï¸ æ¨¡å‹åƒæ•¸è¨­å®š (å«å»ºè­°å€¼)", open=True):
                
                # Group 1: Max New Tokens
                gr.Markdown("**Max New Tokens**", elem_classes="hint-text")
                max_tokens_slider = gr.Slider(minimum=128, maximum=4096, value=1024, step=128, label=None)
                
                gr.HTML("<hr style='margin: 10px 0;'>")
                
                # Group 2: Temperature
                gr.Markdown("**Temperature**", elem_classes="hint-text")
                temp_slider = gr.Slider(minimum=0.1, maximum=1.0, value=0.1, step=0.1, label=None)
                
                gr.HTML("<hr style='margin: 10px 0;'>")
                
                # Group 3: Top-p
                gr.Markdown("**Top-p**", elem_classes="hint-text")
                top_p_slider = gr.Slider(minimum=0.1, maximum=1.0, value=0.9, step=0.05, label=None)

            with gr.Row():
                clear_btn = gr.Button("Clear (æ¸…é™¤)")
                submit_btn = gr.Button("Submit (æäº¤åˆ†æ)", variant="primary")
            
        # å³å´ï¼šè¼¸å‡º
        with gr.Column(scale=1):
            output_display = gr.Textbox(
                label="Generation Result (åˆ†æçµæœ)", 
                placeholder="ç­‰å¾…æ¨¡å‹ç”Ÿæˆ...",
                lines=25,
                interactive=False,
                show_copy_button=True
            )
            flag_btn = gr.Button("Flag (ç´€éŒ„/æ¨™è¨˜å•é¡Œ)")

    # ç¶å®šåŠŸèƒ½
    submit_btn.click(
        fn=generate_security_analysis, 
        inputs=[input_box, max_tokens_slider, temp_slider, top_p_slider], 
        outputs=output_display
    )
    clear_btn.click(fn=lambda: ["", ""], outputs=[input_box, output_display])

# å•Ÿå‹•æœå‹™
demo.launch(share=True, debug=True)