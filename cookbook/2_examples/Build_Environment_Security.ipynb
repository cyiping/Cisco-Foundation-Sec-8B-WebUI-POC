{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AI5ozM2h2Nva"
   },
   "source": [
    "# Use Case Description\n",
    "\n",
    "Automate security monitoring and guidance during the software development lifecycle (SDLC) by embedding a GitHub Action that triggers on pull requests (PRs). The system summarizes changes, evaluates them for security risks, and provides actionable recommendations to reviewers. This turns every PR into an opportunity for proactive security posture improvement — not just static scanning, but contextual reasoning.\n",
    "\n",
    "## Model used for this use case\n",
    "Both Instruct Model and Reasoning Model would be suitable for this task. In this example, we used Instruct Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPloud0h2LI9"
   },
   "source": [
    "## SetUp\n",
    "\n",
    "The setup scripts below are essentially the same as those in the [Quickstart (Instruct Model)](https://github.com/RobustIntelligence/foundation-ai-cookbook/blob/main/1_quickstarts/Preview_Quickstart_instruct_model.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "u5pjZGi32FsI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "def _get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "DEVICE = _get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v46D7fgiBNZF"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd24d42fa094c7eb8088a39acba67a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = \"fdtn-ai/Foundation-Sec-8B-Instruct\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16, # this model's tensor_type is BF16\n",
    "    token=HF_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": None,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"do_sample\": False,\n",
    "    \"use_cache\": True,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def inference(prompt, system_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(inputs, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            **generation_args,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens = False)\n",
    "    \n",
    "    # extract the assistant response part only\n",
    "    match = re.search(r\"<\\|assistant\\|>(.*?)<\\|end_of_text\\|>\", response, re.DOTALL)\n",
    "    \n",
    "    return match.group(1).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Security Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"You are a security expert reviewing changes introduced in a pull request.\"\n",
    "\n",
    "def make_prompt(pr_diff_text):\n",
    "    return f'''Analyze the following code diff and identify:\n",
    "    1. Security-relevant changes\n",
    "    2. Any potential vulnerabilities introduced\n",
    "    3. A clear summary of affected areas\n",
    "    4. Recommended remediations (if needed)\n",
    "    \n",
    "    ## PULL REQUEST DIFF\n",
    "    {pr_diff_text}'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6zZ0aHC5qC_"
   },
   "source": [
    "In this sample PR Diff, Replaced use of `eval()` with `ast.literal_eval()` — this is a significant security improvement. `eval()` can execute arbitrary code, whereas `ast.literal_eval()` safely evaluates strings with Python literals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_diff_text = \"\"\"\n",
    "diff --git a/app/utils.py b/app/utils.py\n",
    "index e69de29..b25a3c0 100644\n",
    "--- a/app/utils.py\n",
    "+++ b/app/utils.py\n",
    "@@ def process_input(data):\n",
    "-    return eval(data)\n",
    "+    import ast\n",
    "+    return ast.literal_eval(data)\n",
    "\n",
    "diff --git a/.github/workflows/deploy.yml b/.github/workflows/deploy.yml\n",
    "index a1b2c3d..d4e5f6g 100644\n",
    "--- a/.github/workflows/deploy.yml\n",
    "+++ b/.github/workflows/deploy.yml\n",
    "@@ steps:\n",
    "-      run: npm run deploy\n",
    "+      run: |\n",
    "+        set -e\n",
    "+        npm run lint\n",
    "+        npm test\n",
    "+        npm run deploy\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Analysis:\n",
       "\n",
       "#### 1. **Security-Relevant Changes**\n",
       "\n",
       "**In `app/utils.py`:**\n",
       "The change from using Python's built-in `eval` function to use `ast.literal_eval` for processing input data is significant.\n",
       "\n",
       "*Before:* \n",
       "```python\n",
       "def process_input(data):\n",
       "    return eval(data) # This can evaluate arbitrary expressions, which poses serious risks if untrusted or malicious data is passed as an argument.\n",
       "```\n",
       "\n",
       "*After:*\n",
       "```python\n",
       "import ast\n",
       "def process_input(data):\n",
       "    return ast.literal_eval(data) # Safer than 'eval' because it only evaluates strings that could be assigned to names according to the syntax rules; does not execute statements or expressions with side effects.\n",
       "```\n",
       "This switch reduces the risk associated with executing potentially harmful code due to unsafe evaluation of user-provided inputs.\n",
       "\n",
       "**In `.github/workflows/deploy.yml`:**\n",
       "The introduction of additional commands (`npm run lint`, `npm test`) before running `npm run deploy` adds more robustness to the deployment workflow by ensuring that any errors related to code quality and tests will halt the pipeline early, preventing deployments of broken software.\n",
       "\n",
       "#### 2. **Potential Vulnerabilities Introduced**\n",
       "\n",
       "* The original usage of `eval()` posed a high-risk vulnerability since it allowed execution of arbitrary python code provided through the `data` parameter. This could lead to command injection attacks, information disclosure, denial-of-service (DoS), and other types of attacks depending on how this method was used within the application context.\n",
       "\n",
       "* There were no new obvious vulnerabilities added directly by these changes themselves but rather the removal of one dangerous practice and addition of good practices like testing and linting improve overall maintainability and reduce future risks.\n",
       "\n",
       "#### 3. **Summary of Affected Areas**\n",
       "\n",
       "* **Code Quality Improvement:** The replacement of `eval` with `ast.literal_eval` improves safety when handling string representations of Python objects without allowing the execution of arbitrary code.\n",
       "  \n",
       "* **CI/CD Pipeline Enhancement:** The updated GitHub Actions workflow now includes static analysis via linter and unit tests before deploying the application, enhancing the reliability and stability of releases.\n",
       "\n",
       "#### 4. **Recommended Remediations (If Needed)**\n",
       "\n",
       "No direct remediation actions are required based solely on the given diffs unless there are further contexts we're unaware of. However, here are some general recommendations:\n",
       "\n",
       "* Ensure all parts of your application handle user-supplied data safely, avoiding dynamic execution whenever possible.\n",
       "* Continuously review and update dependencies and their versions to ensure they do not introduce known vulnerabilities into your project.\n",
       "* Maintain comprehensive test coverage to catch regressions and unexpected behavior changes.\n",
       "* Regularly audit third-party libraries and tools for known weaknesses.\n",
       "* Keep CI/CD pipelines rigorous so that they fail fast upon encountering issues such as failing tests or lint warnings.\n",
       "\n",
       "Given the limited scope of the provided diffs, these suggestions focus broadly on maintaining secure coding standards and improving development processes. If deeper integration with the rest of the system is necessary, consider conducting a full code review or performing automated scans for vulnerabilities across the entire codebase."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = inference(make_prompt(pr_diff_text), SYSTEM_PROMPT)\n",
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
